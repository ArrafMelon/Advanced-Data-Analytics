---
title: "Assignment 2"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Please show all the steps and keep the final answer 3-4 decimal places. You should use R for the applied questions, for example, Unit 02, Q1, 9, 10 and Unit 03, 13.  

## Unit 02

Q1. Please finish this question by hand and by R like the note example. Let $x$ and $y$ equal the ACT scores in social science and natural science, respectively, for a student who is applying for admission to a small liberal arts college. A sample of $n = 15$ such students yielded the following data:
\[
x: 32, 23, 23, 23, 26, 30, 17, 20, 17, 18, 26, 16, 21, 24, 30
\]
\[
y: 28, 25, 24, 32, 31, 27, 23, 30, 18, 18, 32, 22, 28, 31, 26
\]

(a) Plot the scatterplot and calculate the correlation.

ANSWER: Firstly, we can initialize x and y vectors and create a scatterplot in R.
```{r}
x <- c(32, 23, 23, 23, 26, 30, 17, 20, 17, 18, 26, 16, 21, 24, 30)
y <- c(28, 25, 24, 32, 31, 27, 23, 30, 18, 18, 32, 22, 28, 31, 26)

plot(x,y)
```

Now, we have to calculate the correlation, where $r$ is the correlation between $X$ and $Y$. $\bar{x},\bar{y}$ are sample means, $s_x,s_y$ are sample standard deviations of $x$ and $y$

$r=\frac{1}{(n-1)s_xs_y}(\sum{x_iy_i-n\bar{x}\bar{y}})$

$\bar{x}=\frac{32+23+23+23+26+30+17+20+17+18+26+16+21+24+30}{15}=23.0667$
$\bar{y}=\frac{28+25+24+32+31+27+23+30+18+18+32+22+28+31+26}{15}=26.3333$
$s_x=\sqrt{\frac{\sum_i^{15}(x_i-\bar{x})^2}{14}}=5.0493$
$s_y=\sqrt{\frac{\sum_i^{15}(y_i-\bar{y})^2}{14}}=4.6547$

$r=\frac{1}{(14)(5.0493)(4.6547)}(\sum{x_iy_i-15(23.0667)(26.3333)})=0.003039128(180.6650334)=0.5491$

CHECKING IN R:
```{r}
x <- c(32, 23, 23, 23, 26, 30, 17, 20, 17, 18, 26, 16, 21, 24, 30)
y <- c(28, 25, 24, 32, 31, 27, 23, 30, 18, 18, 32, 22, 28, 31, 26)

print(cor(x,y))
```
Therefore, we have seen that the correlation between x and y is 0.5491, which indicates a moderate positive correlation between the two variables.

(b) Calculate the least squares regression line for these data.

ANSWER: We can find the least squares regression line by using the formula: $\hat{y_i}=\hat{b_0}+\hat{b_1}x_i$

With the least squares estimates of slope given by: $\hat{\beta_1}=\frac{\sum_{i=1}^n{x_iy_i}-n\bar{x}\bar{y}}{\sum_{i=1}^n{x_i}^2-n\bar{x}^2}$

From (a), $n=15$,$\bar{x}=23.0667$,$\bar{y}=26.3333$,$\sum_{i=1}^nx_iy_i=9292$

$\sum_{i=1}^n{x_i}^2=8338$

$\hat{\beta_1}=\frac{9292-15(23.0667)(26.3333)}{8338-15(23.0667)^2}=0.5062$

Now the intercept is, $\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}=26.3333-0.5062(23.0667)=14.6569$

Therefore, the least squares regression line is: $\hat{y_i}=14.6569+0.5062x_i$

CHECKING IN R:
```{r}
x <- c(32, 23, 23, 23, 26, 30, 17, 20, 17, 18, 26, 16, 21, 24, 30)
y <- c(28, 25, 24, 32, 31, 27, 23, 30, 18, 18, 32, 22, 28, 31, 26)
model <- lm(y~x)
print(model)
```

(c) Calculate and explain $R^2$. Calculate adjusted $R^2$.

ANSWER: SO, $R^2$ is calculated using the formula: $R^2=1-\frac{SSE}{SST}$. Let's calculate $SSE$ and $SST$:

$SSE=\sum_{i=1}^n(y_i-\hat{y_i})^2$, we can caluclate $\hat{y_i}$ using the regression line from (b) and our x values.

```{r}
x <- c(32, 23, 23, 23, 26, 30, 17, 20, 17, 18, 26, 16, 21, 24, 30)
yhat = 14.6569 + 0.5062*x
print(yhat)
```
$SSE=\sum_{i=1}^n(y_i-\hat{y_i})^2 = 211.8864$

$SST=\sum_{i=1}^n(y_i-\bar{y_i})^2=\sum_{i=1}^n(y_i-26.3333)^2=303.3333$

Therefore, $R^2=1-\frac{211.8864}{303.3333}=0.3015$, meaning that approximately 30.15% of the variation in the outcome $Y$ can be explained by that Predictor $X$.

$Adjusted R^2=1-\frac{SSE/n-p-1}{SST/n-1}=1-\frac{211.8864/15-1-1}{303.3333/15-1}=0.2477$

CHECKING IN R:
```{r}
x <- c(32, 23, 23, 23, 26, 30, 17, 20, 17, 18, 26, 16, 21, 24, 30)
y <- c(28, 25, 24, 32, 31, 27, 23, 30, 18, 18, 32, 22, 28, 31, 26)
model <- lm(y~x)
model_summary <- summary(model)
cat("R^2:", model_summary$r.squared, "Adjusted R^2:", model_summary$adj.r.squared, "\n")
```

(d) Under the simple linear regression model assumptions, calculate the 95\% confidence intervals of $\beta_0$ and $\beta_1$.

ANSWER: $\bar{x}=23.0667, \bar{y}=26.3333, SSE=211.8864, S_{xx}=\sum_{i=1}^nx_i^2-n\bar{x}^2=8338-15(23.0667)^2=356.9103$

$\hat{y_i}=14.6569+0.5062x_i$

$MSE=\frac{SSE}{n-2}=\frac{211.8864}{13}=16.2990$

```{r}
t_value <- qt(0.975, df = 13)
print(t_value)
```

$\hat{\beta_1} \pm t_{0.025,13}\times\sqrt{\frac{MSE}{S_{xx}}}= 0.5062 \pm2.1604\times\sqrt{\frac{16.2990}{356.9103}}=(0.0445,0.9679)$

$\hat{\beta_0}\pm t_{0.025,13}\times \sqrt{MSE} \times \sqrt{\frac{1}{15}+\frac{\bar{x}^2}{S_{xx}}}=14.6569 \pm 2.1604 \times 4.0372 \times \sqrt{\frac{1}{15}+\frac{23.0667^2}{356.9103}}=14.6569 \pm 10.8848=(3.7721,25.5417)$

Therefore, our confidence intervals are $(0.0445,0.9679)$ and $(3.7721,25.5417)$ for $\hat{\beta_1}$ and $\hat{\beta_0}$ respectively.

CHECKING IN R:
```{r}
x <- c(32, 23, 23, 23, 26, 30, 17, 20, 17, 18, 26, 16, 21, 24, 30)
y <- c(28, 25, 24, 32, 31, 27, 23, 30, 18, 18, 32, 22, 28, 31, 26)
model <- lm(y ~ x)
confint(model)
```

(e) Conduct a hypothesis test to verify ACT scores in social science is a significant predictor for ACT scores in natural science at 0.05 significance level.

ANSWER: So, equivalently we test $H_0:\beta_1=0$ vs $H_a:\beta_1≠0$

The test statistic is

$T=\frac{\hat{\beta_1}-0}{\sqrt{MSE/S_{xx}}}=\frac{0.5062-0}{\sqrt{16.2990/356.9103}}=2.3688 \sim t(13)$

```{r}
p_value = 2*(1-pt(2.3688,df=13))
print(p_value)
```
P-value = 2P(t(13) > 2.3688) = 0.0340. Since P-value < 0.05 (significance level), we reject $H_0$ and conclude that ACT scores in social sciences is a significant predictor for ACT scores in natural sciences.

CHECKING IN R:
```{r}
x <- c(32, 23, 23, 23, 26, 30, 17, 20, 17, 18, 26, 16, 21, 24, 30)
y <- c(28, 25, 24, 32, 31, 27, 23, 30, 18, 18, 32, 22, 28, 31, 26)
model <- lm(y ~ x)
summary(model)
```
We see that the code verifies that we should reject $H_0$

(f) find the find the 98\% confidence interval for $\mu_{Y|x}$ when $x = 31$, and the prediction Interval for a new observation of $Y$ when $x = 31$. Which interval is wider?

ANSWER: The 98% CI for $µ_{Y|x}$ when $x = 31$:

$\hat{y}=14.6569+0.5062(31)=30.3491$

```{r}
t_value <- qt(0.99, df = 13)
print(t_value)
```

$\hat{y} \pm t_{0.01,13} \times \sqrt{MSE} \times \sqrt{\frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}}}=30.3491 \pm 2.6503 \times \sqrt{16.2990} \times \sqrt{\frac{1}{15}+ \frac{(31-23.0667)^2}{356.9103}}=30.3491 \pm 5.2745=(25.0746,35.6236)$ is the 98% confidence interval for $µ_{Y|x}$ when $x = 31$

Now, for the prediction interval for a new observation of $Y$ when $x=31$:

$\hat{y}=14.6569+0.5062(31)=30.3491$

$\hat{y} \pm t_{0.01,13} \times \sqrt{MSE} \times \sqrt{1+\frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}}}=30.3491 \pm 2.6503 \times \sqrt{16.2990} \times \sqrt{1+\frac{1}{15}+\frac{(31-23.0667)^2}{356.9103}}=30.3491 \pm 11.9292 = (18.4199,42.2783)$ is the Predicion interval for a new observation of $Y$.

In conclusion, we notice that the prediction interval is wider since it accounts for variability of individual observations in the data.

Q2. For the multiple linear regression model $Y = X \beta + \epsilon$, prove that $SST = SSE + SSR$.

ANSWER: 

$SST = \sum_{i=1}^n(y_i-\bar{y})^2=\sum((y_i-\hat{y_i})(\hat{y_i}-\bar{y}))^2=\sum(y_i-\hat{y_i})^2+2\sum(y_i-\hat{y_i})(\hat{y_i}-\bar{y})+\sum(\hat{y_i}-\bar{y_i})^2=SSE+2\sum(y_i-\hat{y_i})(\hat{y_i}-\bar{y})+SSR$

Now, we must show that $2\sum(y_i-\hat{y_i})(\hat{y_i}-\bar{y})=0$

Note that $\hat{y_i}=\hat{\beta_0}+\hat{\beta_1}x_i$

$\sum(y_i-\hat{y_i})(\hat{y_i}-\bar{y})=\sum(y_i-\hat{\beta_0}-\hat{\beta_1}x_i)(\hat{\beta_0}+\hat{\beta_1}x_i-\bar{y})$

Note that $\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}$

$=\sum(y_i-\bar{y}+\hat{\beta_1}\bar{x}-\hat{\beta_1}x_i)(\bar{y}-\hat{\beta_1}\bar{x}+\hat{\beta_1}x_i-\bar{y})=\sum((y_i-\bar{y})-\hat{\beta_1}(x_i-\bar{x}))(\hat{\beta_1}(x_i-\bar{x}))$ Now, lets distribute:

$=\sum \hat{\beta_1}(x_i-\bar{x})(y_i-\bar{y})-\hat{\beta_1}^2(x_i-\bar{x})^2= \hat{\beta_1}\sum(x_i-\bar{x})(y_i-\bar{y})-\hat{\beta_1}^2\sum(x_i-\bar{x})^2$

Note that $S_{xy}=\sum(x_i-\bar{x})(y_i-\bar{y}), S_{xx}=\sum(x_i-\bar{x})^2,\hat{\beta_1}=\frac{S_{xy}}{S_{xx}}$

So we have: $\frac{S_{xy}}{S_{xx}}S_{xy}-\frac{S_{xy}^2}{S_{xx}^2}S_{xx}=\frac{S_{xy}^2}{S_{xx}}-\frac{S_{xy}^2}{S_{xx}}=0$

Therefore, we have proved $SST=SSE+SSR$ since $2\sum(y_i-\hat{y_i})(\hat{y_i}-\bar{y})=0$

Textbook questions 9, 10 from pg 124 - 125.

Q9.(a)
```{r}
library(ISLR2)
Auto <- Auto

plot(Auto)
```

(b)
```{r}
cor(Auto[, sapply(Auto, is.numeric)])
```
(c)
```{r}
lm_f <- lm(mpg ~ . - name, data = Auto)
summary(lm_f)
```
i. We can see that, there is a relationship between the predictors and the response because the p-value < 2.2e-16 < 0.05 (A commonly used p-value threshold)

ii. Using the same p-value threshold as above, we can see that displacement, weight, year, and origin have a statistically significant relationship with the response (siginified by the stars beside the p-value as well.)

iii. the year coefficient tells how the mpg changes as the year increases. the coefficient is a positive value, meaning that as the year increases, the mpg increases by 0.75 miles per gallon.

(d)
```{r}
plot(lm_f)
```

in the residuals plot, there are three identified outliers that are highlighted with numbers, these outliers need to be investigated as to why their predictions differ from the actual values.

In the Residuals vs leverage plot, we can see that point 14 strays quite far from the cluster. This means that the observations prediction values stray far away from the mean predictor. This point should be investigated as to why only this one point has this type of behaviour.

(e)
```{r}
lm_star <- lm(mpg ~ horsepower * weight, data = Auto)
summary(lm_star)
```
```{r}
lm_colon <- lm(mpg ~ horsepower:weight, data = Auto)
summary(lm_colon)
```
We can see that, * has more features as it includes the added features seperately, as well as the : term. This obviously means that the R^2 will be larger. But it is to note that both * and : have the same P-value, stating it as statistically significant relations.

(f)
```{r}
#LOG
lm_log <- lm(mpg ~ log(year) + log(weight) + log(displacement) + log(origin), data = Auto)
summary(lm_log)
```
```{r}
# ROOT
lm_sqrt <- lm(mpg ~ sqrt(year) + sqrt(weight) + sqrt(displacement) + sqrt(origin), data = Auto)
summary(lm_sqrt)
```
```{r}
# SQUARED
lm_sq <- lm(mpg ~ I(year^2) + I(weight^2) + I(displacement^2) + I(origin^2), data = Auto)
summary(lm_sq)
```
From these three transformations and comparing it to our original model, we see that the square root and log has a slight increase in the R^2, while the squared transformation has a slight decrease. All of these transformations bring different aspects into the model which may affect its performance.

10.(a)
```{r}
library(ISLR2)
Carseats <- Carseats
model_10a <- lm (Sales ~ Price+Urban+US, data=Carseats)
summary(model_10a)
```
(b) The intercept coefficient is the predicted sales when price=0, non-urban, and it is non-US, so everything being the default "no" or 0 value.

The price coefficient represents the relationship between price and sale. As the price increases by a unit ($1), the sale typically decreases by a rate of around 0.0545.

Urban is a qualitative variable. We can interpret this by, "Yes" urban areas decrease sales by around 0.02 units compared to a non-urban area.

US is also a qualitative feature. US areas typically have increased sales by around 1.2 units compared to non-US areas.

(c)$\hat{Sales}=13.0434-0.0545Price-0.0219Urban_{Yes}+1.2006US_{Yes}$

For the qualitative variables, if $US=No, US_{Yes}=0$ and if $US=Yes, US_{Yes}=1$ and $Urban=No, Urban_{Yes}=0$ and $Urban=Yes, Urban_{Yes}=1$

(d) We can reject $H_0$ on the variables Price and UsYes. This is because the P-value < 0.05 which means they are statistically significant predictors of the output variable sales.

(e) 
```{r}
model_10e <- lm (Sales ~ Price+US, data=Carseats)
summary(model_10e)
```
(f) both models in (a) and (e) have significant predictors due to the low p-values. On the other hand, the R^2 for both models are around 0.23, which may mean that other factors contribute to the sales as well, as we want a higher R^2 closer to 1. But overall, a good model to start with but needs improvement.

(g)
```{r}
confint(model_10e, level=0.95)
```
(h)
```{r}
plot(model_10e)
```

We can see that, in the residuals vs fitted plot, points 69 and 377 stray far from 0, indicating they may be potential outliers. This is supported by the same points deviating at the ends of the diagonal on the normal Q-Q plot.

There is 1 high leverage point as we can see on the residuals vs leverage plot. as it points very far to the right, high leverage.

## Unit 03

Q1. Suppose we have 3 classes: $Y = 0, 1, 2$. Assuming the feature $X \sim Uniform (0, 1)$. The conditional probability of $Y$

If $x > 0.6$, 
$$
P (Y=0|x) = 0.4,  P (Y=1|x) = 0.3, P (Y=2|x) = 0.3
$$

Alternatively, for $x \le 0.6$,
$$
P (Y=0|x) = 0,  P (Y=1|x) = 0.2, P (Y=2|x) = 0.8
$$

(a) What is the Bayes error rate for $x > 0.6$ and $x \le 0.6$ separately?

Hint: Calculating $\max_j P (Y = j| x < 0.6)$ and $\max_j P (Y = j| x > 0.6)$ separately.

ANSWER: 
for $x>0.6$: $error=1-E(\underset{j}{max}P(Y=j|X))=1-\underset{x}{\sum}\underset{j}{max}P(Y=j|x)p_X(x)=1-0.4(0.4+0.3+0.3)=1-0.4=0.6$

for $x \le 0.6$: $error=1-E(\underset{j}{max}P(Y=j|X))=1-\underset{x}{\sum}\underset{j}{max}P(Y=j|x)p_X(x)=1-0.8(0+0.2+0.8)=1-0.8=0.2$

Therefore, the Bayes error rate for $x > 0.6$ and $x \le 0.6$ are 0.6 and 0.2 respectively

(b) What is the overall Bayes error rate? 

Hint: Using
$$
E (\max_j P (Y = j| X)) = \int_x \max_j P (Y = j| x) f_X (x) dx
$$
$$= \int_0^{0.6} \max_j P (Y = j| x < 0.6) f_X (x) dx + \int_{0.6}^1 \max_j P (Y = j| x > 0.6) f_X (x) dx
$$

ANSWER: $error=1-E(\underset{j}{max}P(Y=j|X))=1-(\int_0^{0.6} \max_j P (Y = j| x < 0.6) f_X (x) dx + \int_{0.6}^1 \max_j P (Y = j| x > 0.6) f_X (x) dx)$

$=1-(\int_0^{0.6}0.8dx+\int_{0.6}^10.4dx)$

$=1-(\Bigg[0.8x \Bigg]_0^{0.6}+\Bigg[0.4x \Bigg]_{0.6}^1)$

$=1-((0.48-0)+(0.4-0.24))=1-(0.48+0.16)=1-0.64=0.36$

Therefore, the overall Bayes error rate is 0.36.

Textbook questions 7 from pg 54

7.(a) ANSWER: 

The Euclidean Distance between an observation and the Test point is given as: 

$D=\sqrt{(X_1-0)^2+(X_2-0)^2+(X_3-0)^2}=\sqrt{X_1^2+X_2^2+X_3^2}$

Observation 1: $D=\sqrt{0^2+3^2+0^2}=3$

Observation 2: $D=\sqrt{2^2+0^2+0^2}=2$

Observation 3: $D=\sqrt{0^2+1^2+3^2}=3.1623$

Observation 4: $D=\sqrt{0^2+1^2+2^2}=2.2361$

Observation 5: $D=\sqrt{-1^2+0^2+1^2}=1.4142$

Observation 6: $D=\sqrt{1^2+1^2+1^2}=1.7321$

(b) ANSWER: For K=1, we will base our predictions on the closest 1-neighbour of the set. Given our data, the smallest distance would be from observation 4, with a distance of 1.4142, which means the predicted class will be Green.

(c) ANSWER: For K=3, we will base our prediction on the closes 3 neighbours of the set. Given our data, the smallest three distances would be: (obs: 5, dist: 1.4142, class: Green), (6, 1.7321, Red), (2, 2, Red). Since the majority class is red, our predicted class with K=3 is Red.

(d) ANSWER: Since the Bayes Decision boundary is highly nonlinear for this data, we would want a small K to account for the various shaped clusters. If we have a higher K, the decision boundary will be smoother which will disregard the non-linearity of the data.

Therefore, we want a small K-value for this data.

Textbook questions 2, 3, 6, 13 from pg 189 - 191

2. ANSWER: To maximize $P_k(x)$, we notice that the denominator is a constant across all classes, so let's only focus on the numerator:

$\pi_kf_k(x)=\pi_k\frac{1}{2\pi\sigma}exp(-\frac{1}{2\sigma^2}(x-\mu_k)^2)$

to maximize $\pi_kf_k(x)$, lets take the logarithm

$log(\pi_kf_k(x))=log(\pi_k\frac{1}{2\pi\sigma}exp(-\frac{1}{2\sigma^2}(x-\mu_k)^2))$

$=log(\pi_k)+log(\frac{1}{2\pi\sigma})+log(exp(-\frac{1}{2\sigma^2}(x-\mu_k)^2))$

the second term is a constant, so we can ignore it

$=-\frac{(x-\mu_k)^2}{2\sigma^2}+log(\pi_k) = -\frac{(x^2-2x\mu_k+\mu_k^2)}{2\sigma^2}+log(\pi_k)$

$=\frac{2x\mu_k-\mu_k^2}{2\sigma^2}-\frac{x^2}{2\sigma^2}+log(\pi_k)$, the second term is a constance so we can ignore it

$=x\frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+log(\pi_k)=\delta_k(x)$

When we maximize $p_k(x)$ we are also maximizing the discriminant function $\delta_k(x)$, therefore the Bayes classifier assigns an observation to the class for which the discriminant function is maximized.

3. ANSWER: Given (4.16): $f_k(x)=\frac{1}{\sqrt{2\pi}\sigma_k}exp(-\frac{1}{2\sigma^2_k}(x-\mu_k)^2)$ and
$X \sim (\mu_k,\sigma_k^2)$

We know that the Bayes classifier classifies a new point according to which density $f_k(x)$ is the highest, so we should maximize:

$f_k(x)\pi_k=P(X=x|Y=k)P(Y=k)=\frac{1}{\sqrt{2\pi}\sigma_k}exp(-\frac{1}{2\sigma^2_k}(x-\mu_k)^2)\pi_k$

to maximize, let's take the logarithm of the function:

$log(f_k(x)\pi_k)=log(\frac{1}{\sqrt{2\pi}\sigma_k}exp(-\frac{1}{2\sigma^2_k}(x-\mu_k)^2)\pi_k)$

$= log(\frac{1}{\sqrt{2\pi}\sigma_k})+log(\pi_k)+log(exp(-\frac{1}{2\sigma^2_k}(x-\mu_k)^2))$

$= log(\frac{1}{\sqrt{2\pi}\sigma_k}) + log(\pi_k) -\frac{1}{2\sigma^2_k}(x-\mu_k)^2$

$= log(\frac{1}{\sqrt{2\pi}\sigma_k}) + log(\pi_k) - \frac{(x^2-2x\mu_k+\mu_k^2)}{2\sigma^2_k} = \delta_k(x)$

We notice that $\delta_k(x)$ has an $x^2$ term in the equation. Therefore, The Bayes classifier is not linear, but in fact quadratic given the assumptions in this problem. 

6.(a) ANSWER: 

Given:$\hat{\beta_0}=-6, \hat{\beta_1}=0.05,\hat{\beta_2}=1$,

$P(Y=1|X_1,X_2)=\frac{e^{\beta_0+\beta_1X_1+\beta_2X_2}}{1+e^{\beta_0+\beta_1X_1+\beta_2X_2}}$

$=\frac{e^{-6+0.05(40)+1(3.5)}}{1+e^{-6+0.05(40)+1(3.5)}}=\frac{e^{-0.5}}{1+e^{-0.5}}=0.3775$

Therefore there is a 0.3775 probability that the student given the variables $X_1$ and $X_2$ get an A in the class.

(b) ANSWER: We can solve for $X_1$ given the equation:

$0.5=\frac{e^{-6+0.05X_1+1(3.5)}}{1+e^{-6+0.05X_1+1(3.5)}}$

$\frac{1}{2}=\frac{e^{-2.5+0.05X_1}}{1+e^{-2.5+0.05X_1}}$

$2 = \frac{1+e^{-2.5+0.05X_1}}{e^{-2.5+0.05X_1}}$

$2 = 1+ \frac{1}{e^{-2.5+0.05X_1}}$

$1 =  \frac{1}{e^{-2.5+0.05X_1}}$

$log(1) = log(\frac{1}{e^{-2.5+0.05X_1}})$

$log(1) = log(1) - log(e^{-2.5+0.05X_1})$

$0 = -2.5+0.05X_1$

$2.5 = 0.05X_1$

$X_1 = 50$

Therefore, the student would have to study 50 hours for a 50% chance of getting an A

13.(a)
```{r}
library(ggplot2)
library(ISLR2)
Weekly <- Weekly

str(Weekly)
summary(Weekly)

plot(Weekly$Volume, type = "l", main = "Weekly Trading Volume", xlab = "Index", ylab = "Volume", col = "green")
plot(Weekly$Lag1, Weekly$Lag5, main = "Lag1 vs Lag5", xlab = "Lag1", ylab = "Lag5", col = "blue", pch = 20)
plot(Weekly$Lag1, type = "l", main = "Weekly 1 week returns", xlab = "Index", ylab = "Returns", col = "Red")
hist(Weekly$Lag1, main = "Histogram of Lag1", xlab = "Lag1", col = "lightblue", border = "black")
```

As we can see, the returns of Lag1 show that the returns mostly deviate around 0% per week. We have also seen that trading volume has increased over time. 

When comparing Lag1 and Lag5, wee see that the percentage return of week 1 vs week 5 are not that different, but there are some outlier points such as the very left blue point on the lag1 vs lag5 plot. That point indicates that there was less than -15% return on week 1, which by week 5 came around 0%.

(b)
```{r}
model_13b <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, 
             data = Weekly, family = binomial)

summary(model_13b)
```


Any feature with a low p value (p < 0.05) are statistically significant, indicated by the stars beside the p-value. We can see that only Lag 2 appears to be statiscally significant.

(c)
```{r}
library(caret)
library(dplyr)
model_13b_hat <- model_13b %>% predict(Weekly, type = "response")
model_13b_hat_class <- ifelse(model_13b_hat > 0.5, "Up", "Down")
model_13b_train <-  table(model_13b_hat_class, Weekly$Direction)
model_13b_train

model_13b_hat_class <- factor(model_13b_hat_class, levels = c("Down", "Up"))
Weekly$Direction <- factor(Weekly$Direction, levels = c("Down", "Up"))
confusionMatrix(model_13b_hat_class, Weekly$Direction)
```

The confusion matrix has states that the model has an accuracy of 56.11%, which is not very high. A lot of actual "Down's" were predicted "Up's" and vice versa.

(d)
```{r}
train_data <- Weekly %>% filter(Year >= 1990 & Year <= 2008)
test_data  <- Weekly %>% filter(Year >= 2009 & Year <= 2010)
model_13d <- glm(Direction ~ Lag2, data = train_data, family = binomial)

test_probs <- predict(model_13d, test_data, type = "response")
test_preds <- ifelse(test_probs > 0.5, "Up", "Down")

test_preds <- factor(test_preds, levels = c("Down", "Up"))
test_data$Direction <- factor(test_data$Direction, levels = c("Down", "Up"))

confusionMatrix(test_preds, test_data$Direction)
```

We see that in testing, the accuracy of predictions are a benchmark of 0.625, which will be compared to other models.

(e)
```{r}
library(MASS)
model_13e <- lda(Direction ~ Lag2, data = train_data)

test_preds <- predict(model_13e, test_data)

test_lda_class <- test_preds$class
test_lda_class <- factor(test_lda_class, levels = c("Down", "Up"))

confusionMatrix(test_lda_class, test_data$Direction)
```


The accuracy has not changed from (d) using the lda.

(f)
```{r}
model_13f <- qda(Direction ~ Lag2, data = train_data)

test_preds <- predict(model_13f, test_data)

test_lda_class <- test_preds$class
test_lda_class <- factor(test_lda_class, levels = c("Down", "Up"))

confusionMatrix(test_lda_class, test_data$Direction)
```


We see that, the accuracy has went down using QDA.

(g)
```{r}
library(class)

X_train <- as.matrix(train_data$Lag2) 
X_test  <- as.matrix(test_data$Lag2)    
Y_train <- train_data$Direction

set.seed(1)
model_13g <- knn(train = X_train, test = X_test, cl = Y_train, k = 1)

knn_pred <- factor(model_13g, levels = c("Down", "Up"))
confusionMatrix(knn_pred, test_data$Direction)
```

As we can see, a KNN with K=1 significantly decreases the models accuracy compared to the other previous models

(i) The best models for this data were the LDA, and the regular logistic regression models which both gave an accuracy of 0.625

(j)
```{r}
# K=2
set.seed(1)
model_13j_knn2 <- knn(train = X_train, test = X_test, cl = Y_train, k = 2)

knn2_pred <- factor(model_13j_knn2, levels = c("Down", "Up"))
confusionMatrix(knn2_pred, test_data$Direction)

# K=3 
set.seed(1)
model_13j_knn3 <- knn(train = X_train, test = X_test, cl = Y_train, k = 3)

knn3_pred <- factor(model_13j_knn3, levels = c("Down", "Up"))
confusionMatrix(knn3_pred, test_data$Direction)

# Adding more features to LDA
model_13j_LDA <- lda(Direction ~ Lag2+Volume, data = train_data)

test_preds2 <- predict(model_13j_LDA, test_data)

test_lda_class2 <- test_preds2$class
test_lda_class2 <- factor(test_lda_class2, levels = c("Down", "Up"))

confusionMatrix(test_lda_class2, test_data$Direction)
```

As we can see with some different variations of KNN, as we increased our K the accuracy surprisingly went up. This is possibly due to the overfitting problem a low K may give. However, increasing the K too much may decrease accuracy at some point since the decision boundary will keep smoothing out.

For our Modified LDA model, it is pretty expected that the accuracy would go down since we have seen in (b) that volume is not a significant predictor in this dataset, only lag2 is.
